{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Evaluation Notebook\n\nThis notebook provides an example of how to evaluate trained machine learning models using the AI/ML Project Template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n                             roc_auc_score, roc_curve, confusion_matrix, classification_report)\nimport joblib\nimport yaml\n\n# Import project modules\nimport sys\nsys.path.append('../src')\n\nfrom data.preprocessing import preprocess_data\nfrom models.evaluation import evaluate_model\n\n# Load configuration\nwith open('../config/development.yaml', 'r') as f:\n    config = yaml.safe_load(f)\n\nprint('Configuration loaded successfully!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load test data\n# Replace 'your_test_data.csv' with the actual path to your test data file\ntest_df = pd.read_csv('../data/processed/your_test_data.csv')\n\n# Separate features and target\nX_test = test_df.drop('target_column', axis=1)  # Replace 'target_column' with your actual target column name\ny_test = test_df['target_column']\n\nprint(f'Test set size: {X_test.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load trained model\n# Replace 'your_model.pkl' with the actual path to your trained model\nmodel = joblib.load('../models/your_model.pkl')\n\nprint('Model loaded successfully!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions\ny_pred = model.predict(X_test)\n\n# For probability predictions (if applicable)\nif hasattr(model, 'predict_proba'):\n    y_pred_proba = model.predict_proba(X_test)[:, 1]  # Assuming binary classification\nelse:\n    y_pred_proba = y_pred\n\nprint('Predictions made successfully!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate evaluation metrics\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1 = f1_score(y_test, y_pred, average='weighted')\n\n# ROC AUC (for binary classification)\nif len(np.unique(y_test)) == 2:\n    roc_auc = roc_auc_score(y_test, y_pred_proba)\nelse:\n    roc_auc = 'N/A (multiclass)'
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display evaluation metrics\nprint('Model Evaluation Metrics')\nprint('=======================')\nprint(f'Accuracy:  {accuracy:.4f}')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall:    {recall:.4f}')\nprint(f'F1-Score:  {f1:.4f}')\n\nif roc_auc != 'N/A (multiclass)':\n    print(f'ROC AUC:   {roc_auc:.4f}')\nelse:\n    print('ROC AUC:   N/A (multiclass)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification report\nprint('\nClassification Report:')\nprint('====================')\nprint(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=np.unique(y_test),\n            yticklabels=np.unique(y_test))\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.show()\n\nprint('Confusion Matrix:')\nprint(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ROC Curve (for binary classification)\nif len(np.unique(y_test)) == 2:\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n    \n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend(loc='lower right')\n    plt.show()\nelse:\n    print('ROC Curve not applicable for multiclass classification')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance (if available)\nif hasattr(model, 'feature_importances_'):\n    feature_names = X_test.columns\n    importances = model.feature_importances_\n    indices = np.argsort(importances)[::-1]\n    \n    plt.figure(figsize=(10, 6))\n    plt.title('Feature Importances')\n    plt.bar(range(min(20, len(importances))), importances[indices[:20]])\n    plt.xticks(range(min(20, len(importances))), [feature_names[i] for i in indices[:20]], rotation=45)\n    plt.tight_layout()\n    plt.show()\n    \n    print('\nTop 10 Feature Importances:')\n    for i in range(min(10, len(indices))):\n        print(f'{feature_names[indices[i]]}: {importances[indices[i]]:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prediction distribution\nplt.figure(figsize=(10, 6))\n\nplt.subplot(1, 2, 1)\nplt.hist(y_pred, bins=20, alpha=0.7, color='blue', edgecolor='black')\nplt.xlabel('Predicted Values')\nplt.ylabel('Frequency')\nplt.title('Distribution of Predictions')\n\nif hasattr(model, 'predict_proba'):\n    plt.subplot(1, 2, 2)\n    plt.hist(y_pred_proba, bins=20, alpha=0.7, color='green', edgecolor='black')\n    plt.xlabel('Prediction Probabilities')\n    plt.ylabel('Frequency')\n    plt.title('Distribution of Prediction Probabilities')\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Error analysis\nerrors = X_test[y_test != y_pred]\nprint(f'\nNumber of misclassified samples: {len(errors)}')\n\nif len(errors) > 0:\n    print('\nFirst 5 misclassified samples:')\n    print(errors.head())\n\n# Correct predictions\ncorrect = X_test[y_test == y_pred]\nprint(f'\nNumber of correctly classified samples: {len(correct)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save evaluation results\nevaluation_results = {\n    'accuracy': accuracy,\n    'precision': precision,\n    'recall': recall,\n    'f1_score': f1\n}\n\nif roc_auc != 'N/A (multiclass)':\n    evaluation_results['roc_auc'] = roc_auc\n\n# Save to CSV\nresults_df = pd.DataFrame([evaluation_results])\nresults_df.to_csv('../models/evaluation_results.csv', index=False)\n\nprint('\nEvaluation results saved to: ../models/evaluation_results.csv')\nprint('Evaluation completed successfully!')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}